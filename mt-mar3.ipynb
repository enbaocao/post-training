{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be34f18f-b29f-4113-bc76-aadf2a33ea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m485.4/485.4 KB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m84.0/84.0 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m104.1/104.1 KB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m139.8/139.8 KB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m897.5/897.5 KB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m342.1/342.1 KB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting huggingface-hub<1.0,>=0.26.0\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m468.0/468.0 KB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m471.6/471.6 KB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m134.8/134.8 KB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m64.9/64.9 KB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m116.3/116.3 KB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.11.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m194.1/194.1 KB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: lxml in /usr/lib/python3/dist-packages (from sacrebleu) (4.8.0)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from sacrebleu) (0.4.4)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow<2.19,>=2.18 in /usr/lib/python3/dist-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/lib/python3/dist-packages (from ipywidgets) (7.31.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/lib/python3/dist-packages (from ipywidgets) (5.1.1)\n",
      "Collecting widgetsnbextension~=4.0.12\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting comm>=0.1.3\n",
      "  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m214.4/214.4 KB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /usr/lib/python3/dist-packages (from sacremoses) (0.17.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses) (8.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/lib/python3/dist-packages (from accelerate>=0.26.0) (2.5.1)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m319.7/319.7 KB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m124.6/124.6 KB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m205.4/205.4 KB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m241.9/241.9 KB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.9.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m146.1/146.1 KB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Installing collected packages: sentencepiece, xxhash, widgetsnbextension, tqdm, tf-keras, tabulate, safetensors, regex, pyarrow, propcache, portalocker, multidict, jupyterlab-widgets, frozenlist, dill, comm, charset-normalizer, async-timeout, aiohappyeyeballs, yarl, sacremoses, sacrebleu, requests, multiprocess, ipywidgets, aiosignal, huggingface-hub, aiohttp, tokenizers, accelerate, transformers, datasets, evaluate\n",
      "Successfully installed accelerate-1.4.0 aiohappyeyeballs-2.4.6 aiohttp-3.11.13 aiosignal-1.3.2 async-timeout-5.0.1 charset-normalizer-3.4.1 comm-0.2.2 datasets-3.3.2 dill-0.3.8 evaluate-0.4.3 frozenlist-1.5.0 huggingface-hub-0.29.1 ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 multidict-6.1.0 multiprocess-0.70.16 portalocker-3.1.1 propcache-0.3.0 pyarrow-19.0.1 regex-2024.11.6 requests-2.32.3 sacrebleu-2.5.1 sacremoses-0.1.1 safetensors-0.5.3 sentencepiece-0.2.0 tabulate-0.9.0 tf-keras-2.18.0 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.49.0 widgetsnbextension-4.0.13 xxhash-3.5.0 yarl-1.18.3\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m61.1/61.1 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting trl\n",
      "  Downloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m318.9/318.9 KB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m374.8/374.8 KB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: joblib in /usr/lib/python3/dist-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/lib/python3/dist-packages (from bert_score) (21.3)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/lib/python3/dist-packages (from bert_score) (2.5.1)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from bert_score) (1.21.5)\n",
      "Requirement already satisfied: transformers>=3.0.0 in ./.local/lib/python3.10/site-packages (from bert_score) (4.49.0)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from bert_score) (3.5.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/lib/python3/dist-packages (from bert_score) (1.3.5)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/lib/python3/dist-packages (from wandb) (4.21.12)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m325.8/325.8 KB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs in /usr/lib/python3/dist-packages (from wandb) (2.5.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from wandb) (5.4.1)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/lib/python3/dist-packages (from wandb) (4.9.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (59.6.0)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m207.6/207.6 KB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/lib/python3/dist-packages (from wandb) (5.9.0)\n",
      "Collecting pydantic<3,>=2.6\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m431.7/431.7 KB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: datasets>=2.21.0 in ./.local/lib/python3.10/site-packages (from trl) (3.3.2)\n",
      "Requirement already satisfied: rich in /usr/lib/python3/dist-packages (from trl) (11.2.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in ./.local/lib/python3.10/site-packages (from trl) (1.4.0)\n",
      "Requirement already satisfied: safetensors in ./.local/lib/python3.10/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in ./.local/lib/python3.10/site-packages (from peft) (0.29.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (19.0.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets>=2.21.0->trl) (3.6.0)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.11.13)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets>=2.21.0->trl) (2024.3.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting typing-extensions<5,>=4.4\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests->bert_score) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->bert_score) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->bert_score) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->bert_score) (1.26.5)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m128.4/128.4 KB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/lib/python3/dist-packages (from rich->trl) (2.11.2)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/lib/python3/dist-packages (from rich->trl) (0.9.1)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/lib/python3/dist-packages (from rich->trl) (0.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets>=2.21.0->trl) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.6)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (0.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, smmap, setproctitle, nltk, docker-pycreds, bitsandbytes, annotated-types, sentry-sdk, pydantic-core, gitdb, pydantic, gitpython, wandb, peft, bert_score, trl\n",
      "Successfully installed annotated-types-0.7.0 bert_score-0.3.13 bitsandbytes-0.45.3 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 nltk-3.9.1 peft-0.14.0 pydantic-2.10.6 pydantic-core-2.27.2 sentry-sdk-2.22.0 setproctitle-1.3.5 smmap-5.0.2 trl-0.15.2 typing-extensions-4.12.2 urllib3-2.3.0 wandb-0.19.7\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets evaluate sacrebleu tf-keras ipywidgets sentencepiece sacremoses \"accelerate>=0.26.0\"\n",
    "!pip install nltk bert_score wandb trl peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d939c80-8966-42fc-8a1d-90e13dba66bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 21:58:42.415400: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741039122.499416    2049 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741039122.523627    2049 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer, \n",
    "    EarlyStoppingCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2963fe5-b7e7-42c2-9e51-102024950230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menbao\u001b[0m (\u001b[33menbao-pok-mon-showdown9713\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/wandb/run-20250303_215926-5lsmz79p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/enbao-pok-mon-showdown9713/medical-translation-sft-rl/runs/5lsmz79p' target=\"_blank\">en-fr-medical-translation</a></strong> to <a href='https://wandb.ai/enbao-pok-mon-showdown9713/medical-translation-sft-rl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/enbao-pok-mon-showdown9713/medical-translation-sft-rl' target=\"_blank\">https://wandb.ai/enbao-pok-mon-showdown9713/medical-translation-sft-rl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/enbao-pok-mon-showdown9713/medical-translation-sft-rl/runs/5lsmz79p' target=\"_blank\">https://wandb.ai/enbao-pok-mon-showdown9713/medical-translation-sft-rl/runs/5lsmz79p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/enbao-pok-mon-showdown9713/medical-translation-sft-rl/runs/5lsmz79p?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d5db6ee7b80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "wandb.login()  # You'll need to authenticate with your W&B account\n",
    "wandb.init(project=\"medical-translation-sft-rl\", name=\"en-fr-medical-translation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24bfa9cb-3a39-4fb8-8f68-344f22cb7f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161772a159c449a880ae184b4b395653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc66910f05248aab198c4db7c2a99ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ELRC-Medical-V2.py:   0%|          | 0.00/4.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac1cc8f84fd4761b2782545c29b21dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/2.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97febf595e914847af383cf6dcc08a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/13149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"qanastek/ELRC-Medical-V2\", \"en-fr\")\n",
    "\n",
    "# Create a proper train/test split to avoid contamination\n",
    "ds = dataset[\"train\"].train_test_split(test_size=0.2, seed=42) if \"train\" in dataset else dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c04a69-aa3e-4dd1-8200-21b4337705a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split test into validation and test\n",
    "test_valid = ds[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "ds[\"validation\"] = test_valid[\"train\"]\n",
    "ds[\"test\"] = test_valid[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e72ec0-d502-4a4f-b240-32e7c80a32fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3dc3dde4e44fa08f08294054eb7714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49fae90201d4e1ba8988af701c400b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a76850589e1470e990c77d05e330eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093c21fe317a4946902e7cefaa3e51f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a959c4326c7946d7933b613f712cd91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c584e9e1fea64271ab6febbcebc00ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675c813a304e4fc1a306d7b047fb8add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c2764a22a94c55a884eefe7159cd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)  # Create separate base model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1d3bcba-a778-47af-a517-38ad9643c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    en_texts = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    fr_texts = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(en_texts, max_length=128, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(fr_texts, max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b83379dc-943f-400f-97c8-eff5ff7ca972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d06b9fc-6182-471b-ab70-34794737550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments with W&B integration\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_bleu\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"wandb\",  # Enable W&B reporting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89fabbf9-4e22-4488-ac1d-e1c9275ea2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97de9d56172645ed9313e4ffaceb958f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4aa7003d1e4c95975064eab5174a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ubuntu/nltk_data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73a6cd6c51c407083ab79b29c43a34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load multiple metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "068c689e-5154-4947-9846-fab3c3b30371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Post-process text\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[lbl.strip()] for lbl in decoded_labels]\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    meteor_result = meteor_metric.compute(predictions=decoded_preds, references=[l[0] for l in decoded_labels])\n",
    "    bertscore_result = bertscore_metric.compute(predictions=decoded_preds, references=[l[0] for l in decoded_labels], lang=\"fr\")\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "        \"meteor\": meteor_result[\"meteor\"],\n",
    "        \"bertscore_f1\": np.mean(bertscore_result[\"f1\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d39e0b2-8211-412b-8113-e05238b1f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the dataset info to W&B\n",
    "wandb.log({\n",
    "    \"dataset_name\": \"qanastek/ELRC-Medical-V2\",\n",
    "    \"language_pair\": \"en-fr\",\n",
    "    \"train_examples\": len(tokenized_ds[\"train\"]),\n",
    "    \"val_examples\": len(tokenized_ds[\"validation\"]),\n",
    "    \"test_examples\": len(tokenized_ds[\"test\"]),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "950adcab-3b23-49c0-ac5e-ed2e828a610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2049/111762798.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  base_trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate base model before fine-tuning\n",
    "base_trainer = Seq2SeqTrainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4df11f0-0aeb-480a-8fd2-4b8a1bb3b955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating base model before fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d3f2f5528d4b35943816ebcdf3d1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021526c8f2f747d199316919e79e1c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f56c55e6e846869f7a0995f1968551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f50116447e14a48bc19c976bcfa157e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8d7629f3104e83a21b92105d04ae46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model metrics: {'eval_loss': 0.7314205765724182, 'eval_model_preparation_time': 0.0044, 'eval_bleu': 31.23835738621484, 'eval_meteor': 0.5108029440152249, 'eval_bertscore_f1': 0.8172544899095601, 'eval_runtime': 254.0804, 'eval_samples_per_second': 5.176, 'eval_steps_per_second': 0.327}\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating base model before fine-tuning...\")\n",
    "base_eval_results = base_trainer.evaluate()\n",
    "print(\"Base model metrics:\", base_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44a84d72-419d-4cfc-ab17-97c4970538ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log base model metrics to W&B\n",
    "wandb.log({\n",
    "    \"base_bleu\": base_eval_results[\"eval_bleu\"],\n",
    "    \"base_meteor\": base_eval_results[\"eval_meteor\"],\n",
    "    \"base_bertscore\": base_eval_results[\"eval_bertscore_f1\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bd5c9e9-5bf8-40e3-9a77-02af3c2fa875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2049/555213903.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Setup trainer for fine-tuning with early stopping\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a1796c5-4cef-4cdb-b19a-032d39ce0e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting supervised fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3290' max='6580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3290/6580 19:45 < 19:46, 2.77 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Bertscore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.635400</td>\n",
       "      <td>0.549555</td>\n",
       "      <td>35.568437</td>\n",
       "      <td>0.637828</td>\n",
       "      <td>0.862817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>0.536628</td>\n",
       "      <td>41.962783</td>\n",
       "      <td>0.640839</td>\n",
       "      <td>0.867140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>0.538208</td>\n",
       "      <td>44.748357</td>\n",
       "      <td>0.662818</td>\n",
       "      <td>0.875735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.385800</td>\n",
       "      <td>0.539546</td>\n",
       "      <td>43.139672</td>\n",
       "      <td>0.648396</td>\n",
       "      <td>0.869836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.349600</td>\n",
       "      <td>0.540524</td>\n",
       "      <td>42.577094</td>\n",
       "      <td>0.643304</td>\n",
       "      <td>0.865908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3290, training_loss=0.4576783200527759, metrics={'train_runtime': 1185.7569, 'train_samples_per_second': 88.711, 'train_steps_per_second': 5.549, 'total_flos': 1001507064643584.0, 'train_loss': 0.4576783200527759, 'epoch': 5.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune the model with SFT\n",
    "print(\"Starting supervised fine-tuning...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09606a6f-40b2-4d63-a0a7-56a58463ea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./medical_translation_sft/tokenizer_config.json',\n",
       " './medical_translation_sft/special_tokens_map.json',\n",
       " './medical_translation_sft/vocab.json',\n",
       " './medical_translation_sft/source.spm',\n",
       " './medical_translation_sft/target.spm',\n",
       " './medical_translation_sft/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model_path = \"./medical_translation_sft\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be4a13ab-7a96-49f6-b23f-d1323724b620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fine-tuned model...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model metrics: {'eval_loss': 0.5386730432510376, 'eval_bleu': 44.56457202614395, 'eval_meteor': 0.6541183153042133, 'eval_bertscore_f1': 0.8746391176270895, 'eval_runtime': 196.9807, 'eval_samples_per_second': 6.676, 'eval_steps_per_second': 0.421, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model on the test set\n",
    "print(\"Evaluating fine-tuned model...\")\n",
    "eval_results = trainer.evaluate(tokenized_ds[\"test\"])\n",
    "print(\"Fine-tuned model metrics:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9903b692-0989-442f-a65e-fc214321fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log SFT model metrics to W&B\n",
    "wandb.log({\n",
    "    \"sft_bleu\": eval_results[\"eval_bleu\"],\n",
    "    \"sft_meteor\": eval_results[\"eval_meteor\"],\n",
    "    \"sft_bertscore\": eval_results[\"eval_bertscore_f1\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d25ebdc3-cec7-4098-9ae5-764bcb1854d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "English      : Figure 1: Measles Vaccination Coverage Rates in the EU in 2016 (Source: WHO/UNICEF JRF)\n",
      "SFT prediction: Graphique 1: Taux de couverture vaccinale de la rougeole dans l'UE en 2016 (Source: CJJ de l'OMS/UNICEF)\n",
      "Base prediction: Figure 1 : Taux de couverture vaccinale de la rougeole dans l'UE en 2016 (Source : OMS/UNICEF JRF)\n",
      "Reference    : Figure 1: Taux de couverture vaccinale contre la rougeole dans l'UE en 2016 (Source: Formulaire commun OMS/Unicef de notification sur la vaccination)\n",
      "--------------------------------------------------\n",
      "Example 1:\n",
      "English      : ][8: Commission Directive 2006/17/EC of 8 February 2006 implementing Directive 2004/23/EC of the European Parliament and of the Council as regards certain technical requirements for the donation, procurement and testing of human tissues and cells (OJ L38, 9.2.2006, p. 40).\n",
      "SFT prediction: ][8: Directive 2006/17/CE de la Commission du 8 f茅vrier 2006 portant application de la directive 2004/23/CE du Parlement europ茅en et du Conseil en ce qui concerne certaines exigences techniques relatives au don,  l'obtention et au test de tissus et cellules d'origine humaine (JO L 38 du 9.2.2006, p. 40).\n",
      "Base prediction: [8: directive 2006/17/CE de la Commission du 8 f茅vrier 2006 portant application de la directive 2004/23/CE du Parlement europ茅en et du Conseil en ce qui concerne certaines exigences techniques pour le don, l'acquisition et l'essai de tissus et cellules humains (JO L 38 du 9.2.2006, p. 40).\n",
      "Reference    : ][8: Directive 2006/17/CE de la Commission du 8 f茅vrier 2006 portant application de la directive 2004/23/CE du Parlement europ茅en et du Conseil concernant certaines exigences techniques relatives au don,  l'obtention et au contr么le de tissus et de cellules d'origine humaine (JO L 38 du 9.2.2006, p. 40).\n",
      "--------------------------------------------------\n",
      "Example 2:\n",
      "English      : The non-recovered amounts in 2020 will be cleared at programme closure.\n",
      "SFT prediction: Les montants non recouvr茅s en 2020 seront apur茅s lors de la cl么ture du programme.\n",
      "Base prediction: Les montants non recouvr茅s en 2020 seront d茅pass茅s au moment de la cl么ture du programme.\n",
      "Reference    : Les montants non r茅cup茅r茅s en 2020 seront apur茅s  la cl么ture des programmes.\n",
      "--------------------------------------------------\n",
      "Example 3:\n",
      "English      : We need to protect workers from unemployment and loss of income where possible, as they should not become victim of the outbreak.\n",
      "SFT prediction: Nous devons prot茅ger les travailleurs contre le ch么mage et la perte de revenus lorsque cela est possible, car ils ne devraient pas 锚tre victimes de l'茅pid茅mie.\n",
      "Base prediction: Nous devons prot茅ger les travailleurs contre le ch么mage et la perte de revenus dans la mesure du possible, car ils ne devraient pas 锚tre victimes de l'茅pid茅mie.\n",
      "Reference    : Nous devons prot茅ger les travailleurs du ch么mage et de la perte de revenu lorsque cela est possible, pour ne pas en faire des victimes collat茅rales de l'茅pid茅mie.\n",
      "--------------------------------------------------\n",
      "Example 4:\n",
      "English      : The Commission will hold a European conference by the end of 2016 to gather feedback.\n",
      "SFT prediction: La Commission organisera une conf茅rence europ茅enne sur la question de savoir comment.\n",
      "Base prediction: La Commission organisera une conf茅rence europ茅enne en vue de recueillir des informations.\n",
      "Reference    : La Commission organisera une conf茅rence europ茅enne avant la fin de 2016 afin de recueillir des informations en retour.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get a few examples from the test set\n",
    "sample_count = 5\n",
    "sample_original = ds[\"test\"].select(range(sample_count))\n",
    "sample_tokenized = tokenized_ds[\"test\"].select(range(sample_count))\n",
    "\n",
    "# Get predictions from the fine-tuned model\n",
    "ft_predictions = trainer.predict(sample_tokenized)\n",
    "ft_decoded_preds = tokenizer.batch_decode(ft_predictions.predictions, skip_special_tokens=True)\n",
    "\n",
    "# Get predictions from the base model\n",
    "base_predictions = base_trainer.predict(sample_tokenized)\n",
    "base_decoded_preds = tokenizer.batch_decode(base_predictions.predictions, skip_special_tokens=True)\n",
    "\n",
    "# Print comparisons\n",
    "for i, example in enumerate(sample_original):\n",
    "    english_text = example[\"translation\"][\"en\"]\n",
    "    ref_text = example[\"translation\"][\"fr\"]\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"English      :\", english_text)\n",
    "    print(\"SFT prediction:\", ft_decoded_preds[i])\n",
    "    print(\"Base prediction:\", base_decoded_preds[i])\n",
    "    print(\"Reference    :\", ref_text)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Log examples to W&B\n",
    "    wandb.log({\n",
    "        f\"example_{i}\": wandb.Table(\n",
    "            columns=[\"English\", \"SFT Prediction\", \"Base Prediction\", \"Reference\"],\n",
    "            data=[[english_text, ft_decoded_preds[i], base_decoded_preds[i], ref_text]]\n",
    "        )\n",
    "    })\n",
    "\n",
    "# Create a comparison table in W&B\n",
    "comparison_table = wandb.Table(\n",
    "    columns=[\"Example\", \"English\", \"SFT Prediction\", \"Base Prediction\", \"Reference\"]\n",
    ")\n",
    "\n",
    "for i, example in enumerate(sample_original):\n",
    "    english_text = example[\"translation\"][\"en\"]\n",
    "    ref_text = example[\"translation\"][\"fr\"]\n",
    "    comparison_table.add_data(\n",
    "        i, english_text, ft_decoded_preds[i], base_decoded_preds[i], ref_text\n",
    "    )\n",
    "\n",
    "wandb.log({\"translation_comparison\": comparison_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74928b0-9baf-4cd9-af46-ded92f2e4fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
